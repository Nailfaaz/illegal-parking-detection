{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0fe155",
   "metadata": {},
   "source": [
    "# YOLO Model Comparison Experiments\n",
    "\n",
    "## Automated Testing Across All YOLOv8 Models\n",
    "\n",
    "This notebook automatically runs parking detection experiments across all YOLOv8 model variants:\n",
    "- **yolov8n.pt** (Nano) - Fastest, least accurate\n",
    "- **yolov8s.pt** (Small) - Balanced speed/accuracy\n",
    "- **yolov8m.pt** (Medium) - Good accuracy\n",
    "- **yolov8l.pt** (Large) - High accuracy\n",
    "- **yolov8x.pt** (Extra Large) - Highest accuracy, slowest\n",
    "\n",
    "## Features:\n",
    "- Automatic model downloading if missing\n",
    "- Comprehensive performance comparison\n",
    "- Tracking issue analysis\n",
    "- Consolidated results export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the ParkingDetector class from the main notebook\n",
    "exec(open('parking-detection.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparison:\n",
    "    def __init__(self, video_path):\n",
    "        self.video_path = video_path\n",
    "        self.video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "        # Define all YOLOv8 models to test\n",
    "        self.models = [\n",
    "            'yolov8n.pt',  # Nano\n",
    "            'yolov8s.pt',  # Small  \n",
    "            'yolov8m.pt',  # Medium\n",
    "            'yolov8l.pt',  # Large\n",
    "            'yolov8x.pt'   # Extra Large\n",
    "        ]\n",
    "        \n",
    "        self.model_dir = '../yolo-model'\n",
    "        self.results = []\n",
    "        self.comparison_id = f\"comparison_{self.video_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"Model Comparison Setup:\")\n",
    "        print(f\"Video: {self.video_path}\")\n",
    "        print(f\"Models to test: {len(self.models)}\")\n",
    "        print(f\"Comparison ID: {self.comparison_id}\")\n",
    "        \n",
    "    def download_missing_models(self):\n",
    "        \"\"\"Download any missing YOLO models\"\"\"\n",
    "        print(\"\\nChecking for missing models...\")\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            model_path = os.path.join(self.model_dir, model_name)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"Downloading {model_name}...\")\n",
    "                try:\n",
    "                    # Create model directory if it doesn't exist\n",
    "                    os.makedirs(self.model_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Download model (YOLO will handle this automatically)\n",
    "                    temp_model = YOLO(model_name)\n",
    "                    \n",
    "                    # Move to our model directory\n",
    "                    import shutil\n",
    "                    source_path = os.path.join(os.getcwd(), model_name)\n",
    "                    if os.path.exists(source_path):\n",
    "                        shutil.move(source_path, model_path)\n",
    "                        print(f\"Downloaded and moved {model_name} to {model_path}\")\n",
    "                    else:\n",
    "                        print(f\"Model {model_name} downloaded to default location\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to download {model_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"Found {model_name}\")\n",
    "                \n",
    "    def run_single_experiment(self, model_name, show_video=False):\n",
    "        \"\"\"Run experiment with a single model\"\"\"\n",
    "        model_path = os.path.join(self.model_dir, model_name)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing Model: {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            detector = ParkingDetector(model_path, self.video_path)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            success = detector.run_detection(show_video=show_video)\n",
    "            total_experiment_time = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                # Save individual results\n",
    "                output_dir = detector.save_results()\n",
    "                \n",
    "                # Extract key metrics for comparison\n",
    "                stats = detector.results['detection_stats']\n",
    "                perf = detector.results['performance']\n",
    "                \n",
    "                result = {\n",
    "                    'model_name': model_name.replace('.pt', ''),\n",
    "                    'model_path': model_path,\n",
    "                    'experiment_id': detector.results['experiment_id'],\n",
    "                    'output_dir': output_dir,\n",
    "                    'total_experiment_time': total_experiment_time,\n",
    "                    \n",
    "                    # Performance metrics\n",
    "                    'avg_fps': perf['avg_fps'],\n",
    "                    'min_fps': perf['min_fps'],\n",
    "                    'max_fps': perf['max_fps'],\n",
    "                    \n",
    "                    # Detection stats\n",
    "                    'total_frames': stats['total_frames'],\n",
    "                    'total_detections': stats['total_detections'],\n",
    "                    'parked_vehicles': stats['unique_parked'],\n",
    "                    'processing_time': stats['processing_time'],\n",
    "                    \n",
    "                    # Tracking quality\n",
    "                    'tracking_gaps': stats.get('tracking_gaps_total', 0),\n",
    "                    'vehicles_with_issues': stats.get('vehicles_with_tracking_issues', 0),\n",
    "                    'tracking_quality_score': self.calculate_tracking_quality(stats),\n",
    "                    \n",
    "                    # Overall score\n",
    "                    'overall_score': self.calculate_overall_score(perf, stats),\n",
    "                    'success': True\n",
    "                }\n",
    "                \n",
    "                print(f\"SUCCESS: {model_name}\")\n",
    "                print(f\"  Avg FPS: {perf['avg_fps']:.1f}\")\n",
    "                print(f\"  Parked Vehicles: {stats['unique_parked']}\")\n",
    "                print(f\"  Tracking Issues: {stats.get('tracking_gaps_total', 0)}\")\n",
    "                \n",
    "            else:\n",
    "                result = {\n",
    "                    'model_name': model_name.replace('.pt', ''),\n",
    "                    'model_path': model_path,\n",
    "                    'success': False,\n",
    "                    'error': 'Detection failed'\n",
    "                }\n",
    "                print(f\"FAILED: {model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'model_name': model_name.replace('.pt', ''),\n",
    "                'model_path': model_path,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"ERROR: {model_name} - {e}\")\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def calculate_tracking_quality(self, stats):\n",
    "        \"\"\"Calculate tracking quality score (0-100)\"\"\"\n",
    "        total_detections = stats.get('total_detections', 1)\n",
    "        tracking_gaps = stats.get('tracking_gaps_total', 0)\n",
    "        \n",
    "        # Higher score = fewer tracking issues\n",
    "        if total_detections == 0:\n",
    "            return 0\n",
    "            \n",
    "        gap_ratio = tracking_gaps / total_detections\n",
    "        quality_score = max(0, 100 - (gap_ratio * 100))\n",
    "        return round(quality_score, 2)\n",
    "        \n",
    "    def calculate_overall_score(self, perf, stats):\n",
    "        \"\"\"Calculate overall model performance score (0-100)\"\"\"\n",
    "        # Weighted scoring: FPS (30%), Tracking Quality (40%), Detection Count (30%)\n",
    "        \n",
    "        fps_score = min(100, (perf['avg_fps'] / 30) * 100)  # Normalize to 30 FPS max\n",
    "        tracking_score = self.calculate_tracking_quality(stats)\n",
    "        \n",
    "        # Detection efficiency (detections per frame)\n",
    "        detection_efficiency = stats.get('total_detections', 0) / max(1, stats.get('total_frames', 1))\n",
    "        detection_score = min(100, detection_efficiency * 500)  # Scale appropriately\n",
    "        \n",
    "        overall = (fps_score * 0.3) + (tracking_score * 0.4) + (detection_score * 0.3)\n",
    "        return round(overall, 2)\n",
    "        \n",
    "    def run_all_experiments(self, show_video=False):\n",
    "        \"\"\"Run experiments with all models\"\"\"\n",
    "        print(f\"Starting comparison with {len(self.models)} models...\")\n",
    "        print(f\"Video display: {'Enabled' if show_video else 'Disabled (faster)'}\")\n",
    "        \n",
    "        # Download missing models first\n",
    "        self.download_missing_models()\n",
    "        \n",
    "        # Run experiments\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, model_name in enumerate(self.models, 1):\n",
    "            print(f\"\\n[{i}/{len(self.models)}] Testing {model_name}...\")\n",
    "            \n",
    "            result = self.run_single_experiment(model_name, show_video)\n",
    "            self.results.append(result)\n",
    "            \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ALL EXPERIMENTS COMPLETED in {total_time:.1f}s\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return self.results\n",
    "        \n",
    "    def save_comparison_results(self):\n",
    "        \"\"\"Save consolidated comparison results\"\"\"\n",
    "        output_dir = f\"../output/comparisons/{self.comparison_id}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save detailed JSON results\n",
    "        json_file = f\"{output_dir}/comparison_results.json\"\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'comparison_id': self.comparison_id,\n",
    "                'video_path': self.video_path,\n",
    "                'video_name': self.video_name,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'models_tested': len(self.models),\n",
    "                'results': self.results\n",
    "            }, f, indent=2, default=str)\n",
    "            \n",
    "        # Create comparison CSV\n",
    "        successful_results = [r for r in self.results if r.get('success', False)]\n",
    "        \n",
    "        if successful_results:\n",
    "            df = pd.DataFrame(successful_results)\n",
    "            \n",
    "            # Select key columns for comparison\n",
    "            comparison_cols = [\n",
    "                'model_name', 'avg_fps', 'total_detections', 'parked_vehicles',\n",
    "                'tracking_gaps', 'vehicles_with_issues', 'tracking_quality_score',\n",
    "                'overall_score', 'processing_time', 'total_experiment_time'\n",
    "            ]\n",
    "            \n",
    "            comparison_df = df[comparison_cols].copy()\n",
    "            comparison_df = comparison_df.sort_values('overall_score', ascending=False)\n",
    "            \n",
    "            csv_file = f\"{output_dir}/model_comparison.csv\"\n",
    "            comparison_df.to_csv(csv_file, index=False)\n",
    "            \n",
    "            print(f\"\\nComparison results saved:\")\n",
    "            print(f\"  Directory: {output_dir}\")\n",
    "            print(f\"  Files: comparison_results.json, model_comparison.csv\")\n",
    "            \n",
    "            return output_dir, comparison_df\n",
    "        else:\n",
    "            print(f\"No successful experiments to save comparison\")\n",
    "            return output_dir, None\n",
    "            \n",
    "    def display_results(self):\n",
    "        \"\"\"Display comparison results in a nice format\"\"\"\n",
    "        successful_results = [r for r in self.results if r.get('success', False)]\n",
    "        \n",
    "        if not successful_results:\n",
    "            print(\"No successful experiments to display\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nMODEL COMPARISON RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Video: {self.video_name}\")\n",
    "        print(f\"Models tested: {len(successful_results)}/{len(self.models)}\")\n",
    "        print()\n",
    "        \n",
    "        # Sort by overall score\n",
    "        sorted_results = sorted(successful_results, key=lambda x: x['overall_score'], reverse=True)\n",
    "        \n",
    "        # Display ranking\n",
    "        print(f\"{'Rank':<4} {'Model':<8} {'Score':<6} {'FPS':<6} {'Parked':<7} {'Gaps':<5} {'Quality':<8}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        for i, result in enumerate(sorted_results, 1):\n",
    "            print(f\"{i:<4} {result['model_name']:<8} {result['overall_score']:<6} \"\n",
    "                  f\"{result['avg_fps']:<6.1f} {result['parked_vehicles']:<7} \"\n",
    "                  f\"{result['tracking_gaps']:<5} {result['tracking_quality_score']:<8.1f}\")\n",
    "                  \n",
    "        # Show detailed stats for top performer\n",
    "        best_model = sorted_results[0]\n",
    "        print(f\"\\nBEST PERFORMER: {best_model['model_name']}\")\n",
    "        print(f\"  Overall Score: {best_model['overall_score']}\")\n",
    "        print(f\"  Average FPS: {best_model['avg_fps']:.1f}\")\n",
    "        print(f\"  Total Detections: {best_model['total_detections']}\")\n",
    "        print(f\"  Parked Vehicles: {best_model['parked_vehicles']}\")\n",
    "        print(f\"  Tracking Quality: {best_model['tracking_quality_score']:.1f}\")\n",
    "        print(f\"  Processing Time: {best_model['processing_time']:.1f}s\")\n",
    "        \n",
    "        return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81815f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON CONFIGURATION\n",
    "\n",
    "# Video path for testing\n",
    "video_path = '../dataset/video_1_cut_03-57_13-47.mov'\n",
    "\n",
    "# Create comparison instance\n",
    "comparison = ModelComparison(video_path)\n",
    "\n",
    "print(\"Ready to run model comparison!\")\n",
    "print(\"\\nTo start the comparison, run the next cell.\")\n",
    "print(\"Note: This will take significant time as it tests all 5 models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN MODEL COMPARISON\n",
    "\n",
    "print(\"Starting automated model comparison...\")\n",
    "print(\"This will test all YOLOv8 models from nano to extra-large\")\n",
    "print()\n",
    "\n",
    "# Run all experiments (set show_video=True if you want to see each one)\n",
    "results = comparison.run_all_experiments(show_video=False)\n",
    "\n",
    "# Save consolidated results\n",
    "output_dir, comparison_df = comparison.save_comparison_results()\n",
    "\n",
    "# Display results summary\n",
    "ranking = comparison.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAILED RESULTS ANALYSIS\n",
    "\n",
    "if 'comparison_df' in locals() and comparison_df is not None:\n",
    "    print(\"\\nDETAILED ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(\"\\nPERFORMANCE METRICS:\")\n",
    "    print(f\"Fastest Model: {comparison_df.loc[comparison_df['avg_fps'].idxmax(), 'model_name']} \"\n",
    "          f\"({comparison_df['avg_fps'].max():.1f} FPS)\")\n",
    "    print(f\"Most Accurate: {comparison_df.loc[comparison_df['total_detections'].idxmax(), 'model_name']} \"\n",
    "          f\"({comparison_df['total_detections'].max()} detections)\")\n",
    "    print(f\"Best Tracking: {comparison_df.loc[comparison_df['tracking_quality_score'].idxmax(), 'model_name']} \"\n",
    "          f\"({comparison_df['tracking_quality_score'].max():.1f} quality score)\")\n",
    "    \n",
    "    # Speed vs Accuracy trade-off\n",
    "    print(f\"\\nSPEED vs ACCURACY TRADE-OFF:\")\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        speed_category = \"Fast\" if row['avg_fps'] > 15 else \"Medium\" if row['avg_fps'] > 10 else \"Slow\"\n",
    "        accuracy_category = \"High\" if row['total_detections'] > comparison_df['total_detections'].mean() else \"Low\"\n",
    "        print(f\"  {row['model_name']}: {speed_category} speed, {accuracy_category} accuracy\")\n",
    "    \n",
    "    # Tracking issues analysis\n",
    "    print(f\"\\nTRACKING ISSUES ANALYSIS:\")\n",
    "    total_gaps = comparison_df['tracking_gaps'].sum()\n",
    "    total_vehicles_with_issues = comparison_df['vehicles_with_issues'].sum()\n",
    "    \n",
    "    if total_gaps > 0:\n",
    "        print(f\"Total tracking gaps across all models: {total_gaps}\")\n",
    "        print(f\"Models with fewest tracking issues:\")\n",
    "        sorted_by_gaps = comparison_df.sort_values('tracking_gaps')\n",
    "        for _, row in sorted_by_gaps.head(3).iterrows():\n",
    "            print(f\"  {row['model_name']}: {row['tracking_gaps']} gaps\")\n",
    "    else:\n",
    "        print(\"No tracking issues detected in any model!\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nRECOMMENDations:\")\n",
    "    best_overall = comparison_df.loc[comparison_df['overall_score'].idxmax()]\n",
    "    fastest = comparison_df.loc[comparison_df['avg_fps'].idxmax()]\n",
    "    most_accurate = comparison_df.loc[comparison_df['total_detections'].idxmax()]\n",
    "    \n",
    "    print(f\"  Best Overall: {best_overall['model_name']} (score: {best_overall['overall_score']})\")\n",
    "    print(f\"  For Real-time: {fastest['model_name']} ({fastest['avg_fps']:.1f} FPS)\")\n",
    "    print(f\"  For Accuracy: {most_accurate['model_name']} ({most_accurate['total_detections']} detections)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No successful experiments to analyze\")\n",
    "    print(\"Make sure to run the comparison first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e712d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT RESULTS FOR FURTHER ANALYSIS\n",
    "\n",
    "if 'comparison' in locals() and comparison.results:\n",
    "    print(\"EXPORT OPTIONS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Show available data\n",
    "    successful_count = len([r for r in comparison.results if r.get('success', False)])\n",
    "    print(f\"Successfully tested: {successful_count}/{len(comparison.models)} models\")\n",
    "    \n",
    "    if successful_count > 0:\n",
    "        print(f\"\\nResults saved to: {output_dir}\")\n",
    "        print(f\"Files available:\")\n",
    "        print(f\"  - comparison_results.json (detailed data)\")\n",
    "        print(f\"  - model_comparison.csv (summary table)\")\n",
    "        \n",
    "        # Show individual experiment directories\n",
    "        print(f\"\\nIndividual experiment results:\")\n",
    "        for result in comparison.results:\n",
    "            if result.get('success', False):\n",
    "                print(f\"  {result['model_name']}: {result.get('output_dir', 'N/A')}\")\n",
    "                \n",
    "        print(f\"\\nYou can now:\")\n",
    "        print(f\"  1. Open CSV files in Excel/Google Sheets\")\n",
    "        print(f\"  2. Use JSON data for custom analysis\") \n",
    "        print(f\"  3. Compare individual experiment details\")\n",
    "        print(f\"  4. Create custom visualizations\")\n",
    "        \n",
    "        # Quick stats for copying\n",
    "        print(f\"\\nQUICK STATS (copy-friendly):\")\n",
    "        if 'comparison_df' in locals() and comparison_df is not None:\n",
    "            print(\"Model,FPS,Detections,Parked,TrackingGaps,OverallScore\")\n",
    "            for _, row in comparison_df.iterrows():\n",
    "                print(f\"{row['model_name']},{row['avg_fps']:.1f},{row['total_detections']},\"\n",
    "                      f\"{row['parked_vehicles']},{row['tracking_gaps']},{row['overall_score']}\")\n",
    "    else:\n",
    "        print(\"No successful experiments to export\")\n",
    "        \n",
    "else:\n",
    "    print(\"No comparison data available\")\n",
    "    print(\"Run the model comparison first!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
